{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgg1FRTSW3dc"
   },
   "source": [
    "# Data Exploration\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Diego Antonio Garc√≠a Padilla\n",
    "\n",
    "**Date:** Oct 29, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7NJ0YPeYDGP"
   },
   "source": [
    "## Enviroment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "W4fFJZEDYJQe",
    "outputId": "0ffe40a3-bb98-4480-b921-b4df866ba5ac"
   },
   "outputs": [],
   "source": [
    "#@title Setup & Environment Verification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=== ENVIRONMENT CHECK ===\")\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"JAVA_HOME: {os.environ.get('JAVA_HOME')}\")\n",
    "print(f\"SPARK_HOME: {os.environ.get('SPARK_HOME')}\")\n",
    "print(f\"Driver Memory: {os.environ.get('SPARK_DRIVER_MEMORY')}\")\n",
    "print(f\"Executor Memory: {os.environ.get('SPARK_EXECUTOR_MEMORY')}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "5bgOQ8cpWmZX",
    "outputId": "53adea74-2017-427c-b585-78f7a992132b"
   },
   "outputs": [],
   "source": [
    "#@title Import Libraries\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Financial data\n",
    "import yfinance as yf\n",
    "\n",
    "# Hugging Face\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Utilities\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ve3NLAzpd2bJ"
   },
   "outputs": [],
   "source": [
    "#@title Start Spark session\n",
    "\n",
    "print(\"=== PRE-FLIGHT CHECK ===\")\n",
    "\n",
    "# Auto-detect JAVA_HOME if not set properly\n",
    "def find_java_home():\n",
    "    try:\n",
    "        # Method 1: Use 'which java' and follow symlinks\n",
    "        java_path = subprocess.check_output(['which', 'java'], text=True).strip()\n",
    "        java_home = os.path.dirname(os.path.dirname(os.path.realpath(java_path)))\n",
    "        return java_home\n",
    "    except:\n",
    "        # Method 2: Common locations\n",
    "        common_paths = [\n",
    "            '/usr/lib/jvm/java-8-openjdk-amd64',\n",
    "            '/usr/lib/jvm/java-8-openjdk-arm64',\n",
    "            '/usr/lib/jvm/default-java',\n",
    "            '/usr/lib/jvm/java-8-openjdk',\n",
    "        ]\n",
    "        for path in common_paths:\n",
    "            if os.path.exists(os.path.join(path, 'bin', 'java')):\n",
    "                return path\n",
    "        return None\n",
    "\n",
    "# Check current JAVA_HOME\n",
    "current_java_home = os.environ.get('JAVA_HOME')\n",
    "print(f\"Current JAVA_HOME: {current_java_home}\")\n",
    "\n",
    "# Verify Java executable exists\n",
    "if current_java_home:\n",
    "    java_bin = os.path.join(current_java_home, 'bin', 'java')\n",
    "    if not os.path.exists(java_bin):\n",
    "        print(f\"‚ö†Ô∏è  Java not found at {java_bin}\")\n",
    "        detected_java_home = find_java_home()\n",
    "        if detected_java_home:\n",
    "            os.environ['JAVA_HOME'] = detected_java_home\n",
    "            print(f\"‚úÖ Auto-detected JAVA_HOME: {detected_java_home}\")\n",
    "        else:\n",
    "            print(\"‚ùå Could not find Java installation\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Java found at {java_bin}\")\n",
    "else:\n",
    "    detected_java_home = find_java_home()\n",
    "    if detected_java_home:\n",
    "        os.environ['JAVA_HOME'] = detected_java_home\n",
    "        print(f\"‚úÖ Auto-detected JAVA_HOME: {detected_java_home}\")\n",
    "\n",
    "print(f\"SPARK_HOME: {os.environ.get('SPARK_HOME', 'NOT SET')}\")\n",
    "\n",
    "# Verify Java is accessible\n",
    "try:\n",
    "    java_version = subprocess.check_output(['java', '-version'], stderr=subprocess.STDOUT)\n",
    "    print(f\"Java: ‚úÖ Available\")\n",
    "except Exception as e:\n",
    "    print(f\"Java: ‚ùå Not available - {e}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    # Stop any existing Spark sessions\n",
    "    SparkSession.builder.getOrCreate().stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create new session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SAPS_Portfolio_Optimizer\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"3g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"3g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"60\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"512m\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"50MB\") \\\n",
    "    .config(\"spark.local.dir\", \"/tmp/spark\") \\\n",
    "    .config(\"spark.eventLog.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"\\n‚úÖ Spark {spark.version} initialized successfully\")\n",
    "print(f\"   Master: {spark.sparkContext.master}\")\n",
    "print(f\"   App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"   Available cores: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"   Spark UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTdrxZmA4tbV"
   },
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYXI8VZp5_If",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Stock Prices from `yfinance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_ZVbrvaPA-A",
    "outputId": "7e6b2361-01dc-4d61-835c-ae591ce37ddc"
   },
   "outputs": [],
   "source": [
    "#@title Get S&P 500 companies and select diversified portfolio\n",
    "\n",
    "TICKERS_NUMBER = 100\n",
    "tickers_file = '../data/selected_tickers.txt'\n",
    "\n",
    "# Check if tickers file already exists\n",
    "if os.path.exists(tickers_file):\n",
    "    print(f\"‚úÖ File '{tickers_file}' already exists. Loading tickers...\")\n",
    "    with open(tickers_file, 'r') as f:\n",
    "        selected_tickers = [line.strip() for line in f.readlines()]\n",
    "    print(f\"Loaded {len(selected_tickers)} tickers from file\")\n",
    "else:\n",
    "    print(\"Fetching S&P 500 companies from Wikipedia...\")\n",
    "    \n",
    "    # URL of the Wikipedia page listing S&P 500 companies\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S&P_500_companies\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "    }\n",
    "    \n",
    "    # Fetch the HTML content\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Read the S&P 500 table\n",
    "    sp500_data = pd.read_html(response.text)[0]\n",
    "    print(f\"Successfully retrieved {len(sp500_data)} S&P 500 companies\")\n",
    "    \n",
    "    # Define sector allocation (100 stocks total)\n",
    "    sector_allocation = {\n",
    "        'Information Technology': 20,\n",
    "        'Financials': 15,\n",
    "        'Health Care': 15,\n",
    "        'Consumer Discretionary': 15,\n",
    "        'Energy': 10,\n",
    "        'Industrials': 10,\n",
    "        'Consumer Staples': 5,\n",
    "        'Real Estate': 5,\n",
    "        'Utilities': 5\n",
    "    }\n",
    "    \n",
    "    # Tickers to exclude\n",
    "    excluded_tickers = ['BRK.B', 'BF.B']\n",
    "    \n",
    "    # Select diversified stocks by sector\n",
    "    selected_tickers = []\n",
    "    for sector, count in sector_allocation.items():\n",
    "        sector_stocks = sp500_data[sp500_data['GICS Sector'] == sector]['Symbol'].tolist()\n",
    "        # Filter out excluded tickers\n",
    "        sector_stocks = [ticker for ticker in sector_stocks if ticker not in excluded_tickers]\n",
    "        selected_tickers.extend(sector_stocks[:count])\n",
    "    \n",
    "    print(f\"\\nTotal tickers selected: {len(selected_tickers)}\")\n",
    "    \n",
    "    # Save for later use\n",
    "    with open(tickers_file, 'w') as f:\n",
    "        f.write('\\n'.join(selected_tickers))\n",
    "    print(f\"‚úÖ Tickers saved to '{tickers_file}'\")\n",
    "    print(f\"Tickers: \", len(selected_tickers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nJRENIIKTs75",
    "outputId": "5ca82f68-1f69-4025-c5ef-392afbf20344"
   },
   "outputs": [],
   "source": [
    "#@title Get stock prices from yfinance\n",
    "\n",
    "# Download price data for selected tickers\n",
    "start_date = '2015-01-01'\n",
    "end_date = '2025-01-01'\n",
    "\n",
    "# Check if file already exists\n",
    "csv_file = '../data/raw/sp500_prices_raw.csv'\n",
    "\n",
    "if os.path.exists(csv_file):\n",
    "    print(f\"‚úÖ File '{csv_file}' already exists. Skipping download.\")\n",
    "else:\n",
    "    print(f\"Downloading data for {len(selected_tickers)} tickers...\")\n",
    "\n",
    "    # Get selected tickers\n",
    "    selected_tickers = pd.read_csv('../data/selected_tickers.txt', header=None)[0].tolist()\n",
    "\n",
    "    # Download all data at once\n",
    "    all_data = yf.download(\n",
    "        selected_tickers,\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "        group_by='ticker',\n",
    "        threads=True,\n",
    "        progress=True\n",
    "    )\n",
    "\n",
    "    print(f\"Downloaded data for {len(selected_tickers)} tickers\")\n",
    "\n",
    "    # Save to CSV for PySpark processing\n",
    "    all_data.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7PZM3nIW65G",
    "outputId": "9170eb50-7e44-452a-fe2d-44ca44951d23"
   },
   "outputs": [],
   "source": [
    "#@title Convert into PySpark dataframe (Part 1)\n",
    "\n",
    "# File\n",
    "csv_file = '../data/raw/sp500_prices_raw.csv'\n",
    "parquet_file = '../data/raw/sp500_prices_parquet.parquet'\n",
    "\n",
    "# Read the multi-index CSV from yfinance\n",
    "price_df_pandas = pd.read_csv(csv_file, header=[0,1], index_col=0, parse_dates=True)\n",
    "\n",
    "# Flatten the multi-index structure to long format\n",
    "price_data_list = []\n",
    "\n",
    "for ticker in price_df_pandas.columns.get_level_values(0).unique():\n",
    "    try:\n",
    "        # Extract data for this ticker\n",
    "        ticker_df = price_df_pandas[ticker].copy()\n",
    "        ticker_df['ticker'] = ticker\n",
    "        ticker_df['date'] = ticker_df.index\n",
    "\n",
    "        # Rename columns to lowercase\n",
    "        ticker_df = ticker_df.rename(columns={\n",
    "            'Open': 'open',\n",
    "            'High': 'high',\n",
    "            'Low': 'low',\n",
    "            'Close': 'close',\n",
    "            'Volume': 'volume'\n",
    "        })\n",
    "\n",
    "        # Select only required columns\n",
    "        ticker_df = ticker_df[['date', 'ticker', 'open', 'high', 'low', 'close', 'volume']]\n",
    "\n",
    "        price_data_list.append(ticker_df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ticker}: {e}\")\n",
    "\n",
    "# Concatenate all ticker dataframes\n",
    "price_df_long = pd.concat(price_data_list, ignore_index=True)\n",
    "\n",
    "# Add adj_close as close for now\n",
    "price_df_long['adj_close'] = price_df_long['close']\n",
    "\n",
    "# FIX: Convert date to string to avoid timestamp precision issues\n",
    "price_df_long['date'] = price_df_long['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"Long format created: {len(price_df_long)} rows\")\n",
    "print(f\"Unique tickers: {price_df_long['ticker'].nunique()}\")\n",
    "\n",
    "# Save as parquet (now with date as string)\n",
    "price_df_long.to_parquet(parquet_file, index=False, engine='pyarrow')\n",
    "print(f\"Saved to {parquet_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7uhSHxw7bCNd",
    "outputId": "52cd8553-2e62-4cac-a4f1-43a5a25449ee"
   },
   "outputs": [],
   "source": [
    "#@title Convert into PySpark dataframe (Part 2)\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, DateType, StringType, DoubleType, LongType\n",
    "\n",
    "# Load parquet with date as string\n",
    "price_spark_df = spark.read.parquet('../data/raw/sp500_prices_parquet.parquet')\n",
    "\n",
    "# Convert string date to proper DateType\n",
    "price_spark_df = price_spark_df \\\n",
    "    .withColumn(\"date\", F.to_date(F.col(\"date\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"open\", F.col(\"open\").cast(\"double\")) \\\n",
    "    .withColumn(\"high\", F.col(\"high\").cast(\"double\")) \\\n",
    "    .withColumn(\"low\", F.col(\"low\").cast(\"double\")) \\\n",
    "    .withColumn(\"close\", F.col(\"close\").cast(\"double\")) \\\n",
    "    .withColumn(\"adj_close\", F.col(\"adj_close\").cast(\"double\")) \\\n",
    "    .withColumn(\"volume\", F.col(\"volume\").cast(\"long\")) \\\n",
    "    .filter(F.col(\"close\").isNotNull()) \\\n",
    "    .orderBy(\"date\", \"ticker\")\n",
    "\n",
    "# Cache for faster operations\n",
    "price_spark_df.cache()\n",
    "\n",
    "print(f\"PySpark DataFrame loaded: {price_spark_df.count():,} rows\")\n",
    "print(\"\\nSchema:\")\n",
    "price_spark_df.printSchema()\n",
    "print(\"\\nSample data:\")\n",
    "price_spark_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sCR8l8k6EzM"
   },
   "source": [
    "### Stock News from `Hugging Face`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "CB9Qfz7H4cCe"
   },
   "outputs": [],
   "source": [
    "#@title Login into Hugging Face\n",
    "\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Try to load from .env file (for local development)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    hf_token = os.getenv('HF_TOKEN')\n",
    "except ImportError:\n",
    "    # If python-dotenv not installed, try Google Colab secrets\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        hf_token = userdata.get('HF_TOKEN')\n",
    "    except ImportError:\n",
    "        # Fall back to environment variable\n",
    "        hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "# Log in using the retrieved token\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"‚úÖ Logged in to Hugging Face\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  HF_TOKEN not found. Please set it in .env file or environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "id": "YOzyOsEZ6Jub",
    "outputId": "a727e4c0-063b-4df5-c3da-7c2314f9fca9"
   },
   "outputs": [],
   "source": [
    "#@title Download FNSPID from Hugging Face (Part 1: Download & Filter)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DOWNLOADING FNSPID - OPTIMIZED VERSION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# File paths\n",
    "raw_csv_file = '../data/raw/fnspid_news_raw.csv'\n",
    "parquet_file = '../data/raw/fnspid_news_filtered.parquet'\n",
    "\n",
    "# Check if already downloaded and filtered\n",
    "if os.path.exists(parquet_file):\n",
    "    print(f\"‚úÖ Filtered parquet file already exists: {parquet_file}\")\n",
    "    print(\"Skipping download and filtering...\")\n",
    "else:\n",
    "    # Step 1: Download with progress tracking\n",
    "    print(\"\\nüì• Step 1: Downloading from Hugging Face...\")\n",
    "    print(\"(This may take several minutes for large files)\")\n",
    "\n",
    "    file_path = hf_hub_download(\n",
    "        repo_id=\"Zihan1004/FNSPID\",\n",
    "        filename=\"Stock_news/All_external.csv\",\n",
    "        repo_type=\"dataset\",\n",
    "        resume_download=True  # Allow resuming if interrupted\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Downloaded to: {file_path}\")\n",
    "\n",
    "    # Check file size\n",
    "    file_size_mb = os.path.getsize(file_path) / (1024**2)\n",
    "    print(f\"File size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "    # Step 2: Load and filter in chunks (memory efficient)\n",
    "    print(\"\\nüìä Step 2: Loading and filtering data in chunks...\")\n",
    "    print(\"This prevents memory overflow by processing incrementally\")\n",
    "\n",
    "    # Load tickers to filter\n",
    "    with open('../data/selected_tickers.txt', 'r') as f:\n",
    "        selected_tickers = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    selected_tickers_set = set(selected_tickers)\n",
    "    print(f\"Filtering for {len(selected_tickers)} tickers\")\n",
    "\n",
    "    # Process in chunks\n",
    "    chunk_size = 50000  # Process 50k rows at a time\n",
    "    filtered_chunks = []\n",
    "    total_rows_processed = 0\n",
    "    total_rows_kept = 0\n",
    "\n",
    "    # First, peek at the columns to find ticker column\n",
    "    sample = pd.read_csv(file_path, nrows=5)\n",
    "    print(f\"\\nAvailable columns: {sample.columns.tolist()}\")\n",
    "\n",
    "    # Find ticker column (could be 'ticker', 'symbol', 'stock', etc.)\n",
    "    ticker_col = None\n",
    "    for col in ['Stock_symbol', 'ticker', 'symbol']:\n",
    "        if col in sample.columns:\n",
    "            ticker_col = col\n",
    "            break\n",
    "\n",
    "    if ticker_col is None:\n",
    "        print(\"‚ö†Ô∏è  Could not find ticker column. Loading all rows...\")\n",
    "        fnspid_df_pandas = pd.read_csv(file_path)\n",
    "    else:\n",
    "        print(f\"Found ticker column: '{ticker_col}'\")\n",
    "        print(\"\\nProcessing chunks...\")\n",
    "\n",
    "        # Read and filter in chunks\n",
    "        with tqdm(desc=\"Processing\", unit=\" rows\") as pbar:\n",
    "            for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "                total_rows_processed += len(chunk)\n",
    "\n",
    "                # Filter for our tickers\n",
    "                filtered_chunk = chunk[chunk[ticker_col].isin(selected_tickers_set)]\n",
    "\n",
    "                if len(filtered_chunk) > 0:\n",
    "                    filtered_chunks.append(filtered_chunk)\n",
    "                    total_rows_kept += len(filtered_chunk)\n",
    "\n",
    "                pbar.update(len(chunk))\n",
    "                pbar.set_postfix({\n",
    "                    'kept': total_rows_kept,\n",
    "                    'processed': total_rows_processed\n",
    "                })\n",
    "\n",
    "        # Combine filtered chunks\n",
    "        if filtered_chunks:\n",
    "            fnspid_df_pandas = pd.concat(filtered_chunks, ignore_index=True)\n",
    "            print(f\"\\n‚úÖ Filtered: {total_rows_kept:,} / {total_rows_processed:,} rows\")\n",
    "            print(f\"   Reduction: {(1 - total_rows_kept/total_rows_processed)*100:.1f}%\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  No matching data found. Loading sample...\")\n",
    "            fnspid_df_pandas = pd.read_csv(file_path, nrows=10000)\n",
    "\n",
    "    # Step 3: Save as parquet for Spark processing\n",
    "    print(f\"\\nüíæ Saving filtered data to: {parquet_file}\")\n",
    "    fnspid_df_pandas.to_parquet(parquet_file, index=False, engine='pyarrow')\n",
    "    print(\"‚úÖ Saved!\")\n",
    "\n",
    "    print(f\"\\nFiltered Pandas DataFrame: {len(fnspid_df_pandas):,} rows\")\n",
    "    print(f\"Columns: {fnspid_df_pandas.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load FNSPID with PySpark (Part 2: Convert to Spark DataFrame)\n",
    "\n",
    "# Load parquet file with Spark\n",
    "fnspid_spark_df = spark.read.parquet('../data/raw/fnspid_news_filtered.parquet')\n",
    "\n",
    "# Apply any necessary transformations\n",
    "# Rename 'Stock_symbol' to 'ticker' for consistency if needed\n",
    "if 'Stock_symbol' in fnspid_spark_df.columns:\n",
    "    fnspid_spark_df = fnspid_spark_df.withColumnRenamed('Stock_symbol', 'ticker')\n",
    "\n",
    "# Convert date column to proper DateType if it exists and is string\n",
    "if 'date' in fnspid_spark_df.columns:\n",
    "    # Check if date needs conversion\n",
    "    date_type = dict(fnspid_spark_df.dtypes)['date']\n",
    "    if date_type == 'string':\n",
    "        fnspid_spark_df = fnspid_spark_df.withColumn(\"date\", F.to_date(F.col(\"date\")))\n",
    "\n",
    "# Filter out null records if necessary\n",
    "fnspid_spark_df = fnspid_spark_df.filter(F.col(\"ticker\").isNotNull())\n",
    "\n",
    "# Order by date and ticker\n",
    "fnspid_spark_df = fnspid_spark_df.orderBy(\"date\", \"ticker\")\n",
    "\n",
    "# Cache for faster operations\n",
    "fnspid_spark_df.cache()\n",
    "\n",
    "# Display results\n",
    "print(\"=\" * 60)\n",
    "print(\"FNSPID DATA LOADED WITH PYSPARK\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PySpark DataFrame loaded: {fnspid_spark_df.count():,} rows\")\n",
    "print(\"\\nSchema:\")\n",
    "fnspid_spark_df.printSchema()\n",
    "print(\"\\nUnique tickers:\")\n",
    "fnspid_spark_df.select(\"ticker\").distinct().orderBy(\"ticker\").show(10)\n",
    "print(\"\\nSample data:\")\n",
    "fnspid_spark_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRC2HIfR5yIw"
   },
   "source": [
    "## Data Exploration\n",
    "\n",
    "Let's use Cisco (`CSCO`) stock to illustrate."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "pYXI8VZp5_If"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
